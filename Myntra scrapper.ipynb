{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c99836a8",
   "metadata": {},
   "source": [
    "# Myntra Product Information Scraper\n",
    "\n",
    "Welcome to the Myntra Product Information Scraper! This Jupyter notebook contains a powerful Python script designed to collect detailed information about any product available on Myntra.\n",
    "\n",
    "## How to Use:\n",
    "\n",
    "1. **Enter Product Details:** In the input fields, provide the name of the product you're interested in and specify the number of pages to scrape.\n",
    "\n",
    "2. **Initiate Scraping:** Click the \"Scrape Data\" button to initiate the web scraping process. The script will navigate through Myntra's pages, collecting brand names, model names, prices, and ratings.\n",
    "\n",
    "3. **Data Cleaning:** The collected data is cleaned, and additional details such as discounted price, original price, discount percentage, and total reviews are extracted.\n",
    "\n",
    "4. **Save Results:** The cleaned data is saved in an Excel file named 'myntra_cleaned_data.xlsx' for your convenience.\n",
    "\n",
    "5. **Explore and Analyze:** Dive into the scraped data to analyze trends, make comparisons, and gather insights about your favorite products on Myntra.\n",
    "\n",
    "Feel free to modify the script and adapt it for different products or purposes. Happy scraping! ðŸš€ðŸ›ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95084e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_collected 0\n",
      "page_collected 1\n",
      "page_collected 2\n",
      "page_collected 3\n",
      "page_collected 4\n",
      "page_collected 5\n",
      "page_collected 6\n",
      "page_collected 7\n",
      "page_collected 8\n",
      "page_collected 9\n",
      "ALL PAGES COLLECTED\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from tkinter import PhotoImage\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "def scrape_data(product_name, num_pages):\n",
    "    # Create a Chrome web driver instance and set up its service\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Define the URL you want to access\n",
    "    url = \"https://www.myntra.com/\"\n",
    "\n",
    "    # Open the specified URL in the web browser\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Click on search\n",
    "    search = driver.find_element(By.XPATH, \"/html/body/div[1]/div/div/header/div[2]/div[3]/input\")\n",
    "    search.click()\n",
    "    time.sleep(3)\n",
    "    search.send_keys(product_name)\n",
    "    time.sleep(3)\n",
    "    search.send_keys(Keys.ENTER)\n",
    "\n",
    "    # Let's collect the data\n",
    "    df_overall = pd.DataFrame(columns=[\"brand_Name\", \"model_name\", \"prices\", \"ratings\"])\n",
    "\n",
    "    for v in range(num_pages):\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        table = soup.find('div', class_='search-searchProductsContainer row-base')\n",
    "\n",
    "        brand_name = [item.text for item in table.find_all(\"h3\", class_='product-brand')]\n",
    "        model = [item.text for item in table.find_all(\"h4\", class_='product-product')]\n",
    "        price = [item.text for item in table.find_all(\"div\", class_='product-price')]\n",
    "        rating = [item.text if item else None for item in table.find_all(\"div\", class_='product-ratingsContainer')]\n",
    "\n",
    "        brand_series = pd.Series(brand_name, name=\"brand_Name\")\n",
    "        model_series = pd.Series(model, name=\"model_name\")\n",
    "        price_series = pd.Series(price, name=\"prices\")\n",
    "        rating_series = pd.Series(rating, name=\"ratings\")\n",
    "\n",
    "        df = pd.concat([brand_series, model_series, price_series, rating_series], axis=1)\n",
    "        df_overall = pd.concat([df_overall, df], ignore_index=True, sort=False)\n",
    "\n",
    "        # Click on the next page\n",
    "        next_button = WebDriverWait(driver, 3).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, '#desktopSearchResults > div.search-searchProductsContainer.row-base > section > div.results-showMoreContainer > ul > li.pagination-next > a'))\n",
    "        ).click()\n",
    "        print(\"page_collected\", v)\n",
    "\n",
    "    print(\"ALL PAGES COLLECTED\")\n",
    "    driver.quit()\n",
    "\n",
    "    # Clean the data\n",
    "    data2 = df_overall\n",
    "    pattern = r'Rs\\. (\\d+)(?:Rs\\. (\\d+))?(?:\\((\\d+)% OFF\\))?(?:\\(Rs\\. (\\d+) OFF\\))?'\n",
    "    extracted = data2['prices'].str.extract(pattern)\n",
    "    extracted.columns = ['discounted_price', 'original_price', 'discount_percentage', 'discount_off']\n",
    "    extracted = extracted.apply(pd.to_numeric, errors='coerce')\n",
    "    data2 = pd.concat([data2, extracted], axis=1)\n",
    "    data2 = data2.drop(columns=['prices'])\n",
    "    data2['Total Reviews'] = data2['ratings'].replace({None: 0})\n",
    "    data2['Total Reviews'] = data2['Total Reviews'].str.replace('k', '', regex=False)\n",
    "#     decimals = data2['Total Reviews'] % 1 != 0\n",
    "#     data2.loc[decimals, 'Total Reviews'] *= 1000\n",
    "#     data2['Total Reviews'] = pd.to_numeric(data2['Total Reviews'], errors='coerce').fillna(0)\n",
    "    \n",
    "    data2['Total Reviews'] = data2['Total Reviews'].apply(lambda x: float(x.replace('k', '')) * 1000 if 'k' in str(x) else x)\n",
    "    data2['Total Reviews'] = pd.to_numeric(data2['Total Reviews'], errors='coerce').fillna(0)\n",
    "    \n",
    "    data2['discount_percentage'].fillna((data2['original_price'] - data2['discounted_price']) / data2['original_price'] * 100,\n",
    "                                         inplace=True)\n",
    "\n",
    "    # Save the cleaned data to an Excel file\n",
    "    data2.to_excel(r'D:\\TeraBoxDownload\\Python\\data collection - myntra\\myntra_cleaned_data.xlsx')\n",
    "\n",
    "    messagebox.showinfo(\"Scraping Complete\", \"Data has been scraped and cleaned!\")\n",
    "\n",
    "\n",
    "def exit_program():\n",
    "    root.destroy()\n",
    "    \n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Myntra Web Scraping\")\n",
    "\n",
    "# Add a description label\n",
    "description_label = tk.Label(root, text=\"Welcome to My Myntra Web Scraping Tool!\\nEnter the product name and the number of pages to scrape.\")\n",
    "description_label.pack(side=tk.TOP, pady=10)\n",
    "\n",
    "# Load the Myntra wallpaper image\n",
    "bg_image = PhotoImage(file=\"D:\\TeraBoxDownload\\Python\\data collection - myntra\\wallpaper.png\")\n",
    "\n",
    "# Set the background image\n",
    "background_label = tk.Label(root, image=bg_image)\n",
    "background_label.place(relwidth=1, relheight=1)\n",
    "\n",
    "# Create labels and entry for product name\n",
    "label_name = tk.Label(root, text=\"Enter Product Name:\", bg='white')\n",
    "label_name.pack(side=tk.LEFT, anchor='center')\n",
    "\n",
    "product_name_entry = tk.Entry(root)\n",
    "product_name_entry.pack(side=tk.LEFT, anchor='center')\n",
    "\n",
    "# Create labels and entry for number of pages\n",
    "label_pages = tk.Label(root, text=\"Enter Number of Pages:\", bg='white')\n",
    "label_pages.pack(side=tk.LEFT, anchor='center')\n",
    "\n",
    "num_pages_entry = tk.Entry(root)\n",
    "num_pages_entry.pack(side=tk.LEFT, anchor='center')\n",
    "\n",
    "# Create a button to trigger web scraping\n",
    "scrape_button = tk.Button(root, text=\"Scrape Data\", command=lambda: scrape_data(product_name_entry.get(), int(num_pages_entry.get())))\n",
    "scrape_button.pack(side=tk.LEFT, anchor='center')\n",
    "\n",
    "# Create an exit button\n",
    "exit_button = tk.Button(root, text=\"Exit\", command=exit_program)\n",
    "exit_button.pack()\n",
    "\n",
    "# Start the GUI event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847b073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
